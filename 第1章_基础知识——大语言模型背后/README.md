
- 第1章 基础知识——大语言模型背后
  - 1.1 自然语言背景
    - 1.1.1 语言是智能的标志
    - 1.1.2 从图灵测试到ChatGPT
  - 1.2 语言模型基础
    - 1.2.1 最小语义单位Token与Embedding
    - 1.2.2 语言模型怎么回事
  - 1.3 ChatGPT基础
    - 1.3.1 最强表示架构Transformer设计与演变
    - 1.3.2 生成语言模型GTP进化与逆袭
    - 1.3.3 利器强化学习RLHF流程与思想
  - 1.4 本章小结

# 第1章 基础知识——大语言模型背后
## 1.1 自然语言背景
### 1.1.1 语言是智能的标志

语言是人类独有的。
机器能否具备使用自然语言同人类沟通交流的能力，就成了机器是否具有人类智能的一条重要标准。

### 1.1.2 从图灵测试到ChatGPT

## 1.2 语言模型基础
### 1.2.1 最小语义单位Token与Embedding
Token：中文里一般是一个字，或者词。英文使用子词。子词可以同时兼顾词表大小和语义表示。
例如 annoyingly （adv. 恼人地）拆分成
```python
["annoying", "##ly"]
# ## 表示和前一个token 是直接拼接的，没有空格。
```

* One-hot独热编码
* 词袋模型（bag of words, BOW）
  * 每个文本表示为一个向量，向量的每一个维度对应一个词，维度的值表示这个词在文本中出现的次数。向量的长度为词表的大小。
  * 存在问题：（1）词表大，向量维度高，稀疏。（2）忽略了顺序，丢失了语义。
* 词向量（词嵌入）：一个token表示成一定数量的小数，稠密的。
* 句子向量：对句子的所有词直接取平均。
* Embedding 表示技术：将任意文本表示成稠密向量的方法。

### 1.2.2 语言模型怎么回事
语言模型（Language Model, LM）:利用自然语言构建的模型。

概率语言模型：下一个Token的概率。通过已有的token，预测接下来的token。

如何给定文本输出对应文本（搜索方法/解码策略）？
- 贪心搜索（greedy search）：只考虑下一步概率最大的词。
- 集束搜索（beam search）：一步多看几个词，看最终句子的概率。

N-GRAM模型：n表示每次用到的上下文token的数量。下一个token根据前n-1个token得到。
- Bi-Gram：人工智能/让 让/世界 世界/变得 变得/更 更/美好
- Tri-Gram：人工智能/让/世界 让/世界/变得 世界/变得/更 变得/更/美好
训练n-gram就是统计频率的过程。


RNN（循环神经网络， recurrent neural network）


总结：
1. 构建（训练）语言模型的过程就是学习词、句内在的“语言关系”；
2. 推理（预测）就是在给定上下文后，让构建的模型根据不同的解码策略输出对应的文本。
3. 无论训练还是预测，都以Token为粒度进行的。

## 1.3 ChatGPT基础
### 1.3.1 最强表示架构Transformer设计与演变
Transformer是一个基于注意力机制的编码器-解码器架构(encoder-decoder)，最重要的核心是自注意力机制（self-attention）。
- 自注意力（self-attention）：自己的每一个token之间的重要性权重。

编码时可以同时利用过去的token和未来的token；解码时不能利用未来的token。

Transformer主要用了两个模块：
- 多头注意力(multi-head attention)：每个头注意到的信息不一样，多头可捕获更多信息。
- 前馈网络(feedback):主要引入非线性变换，帮助模型学习更复杂的语言特征和模式。

解码器内有一个遮盖多头注意力（masked multi-head attention）


- 自然语言理解（natural language understanding, NLU）任务
  - 句子分类
  - token分类
  - 相似匹配
- 自然语言生成（natural language generation, NLG）任务
  - 文本摘要
  - 文本续写
  - 机器翻译
  - 文本改写
  - 文本纠错

BERT使用Transformer的编码器，随机把15%的token盖住，然后利用其他未盖住的token来预测盖住位置的token。


### 1.3.2 生成语言模型GTP进化与逆袭
GPT：生成式预训练Transformer。使用的解码器。

- GPT-1：1.1亿。和bert一样，用的是下游任务微调范式，就是在不同任务上微调预训练模型。针对不同的任务构造不同的输入序列，然后丢给GPT-1获取token或句子的embedding表示，再通过linear+softmax输出结果。
- GPT-2：15亿。扩大规模、零样本。
- GPT-3: 1750亿。


### 1.3.3 利器强化学习RLHF流程与思想
InstructGPT：用强化学习的算法微调一个根据人类反馈来加以改进的语言模型。

InstructGPT的流程：
- 步骤一：有监督微调
- 步骤二：奖励模型
- 步骤三：强化学习



## 1.4 本章小结



