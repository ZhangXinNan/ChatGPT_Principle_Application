
- 第1章 基础知识——大语言模型背后
  - 1.1 自然语言背景
    - 1.1.1 语言是智能的标志
    - 1.1.2 从图灵测试到ChatGPT
  - 1.2 语言模型基础
    - 1.2.1 最小语义单位Token与Embedding
    - 1.2.2 语言模型怎么回事
  - 1.3 ChatGPT基础
    - 1.3.1 最强表示架构Transformer设计与演变
    - 1.3.2 生成语言模型GTP进化与逆袭
    - 1.3.3 利器强化学习RLHF流程与思想
  - 1.4 本章小结

# 第1章 基础知识——大语言模型背后
## 1.1 自然语言背景
### 1.1.1 语言是智能的标志
### 1.1.2 从图灵测试到ChatGPT

## 1.2 语言模型基础
### 1.2.1 最小语义单位Token与Embedding
Token：中文里一般是一个字，或者词。英文使用子词。

* One-hot独热编码
* 词袋模型（bag of words, BOW）
  * 每个文本表示为一个向量，向量的每一个维度对应一个词，维度的值表示这个词在文本中出现的次数。向量的长度为词表的大小。
  * 存在问题：（1）词表大，向量维度高，稀疏。（2）忽略了顺序，丢失了语义。
* 词向量（词嵌入）：一定数量的小数，稠密的。
* Embedding 表示技术

### 1.2.2 语言模型怎么回事
语言模型（Language Model, LM）:利用自然语言构建的模型。

如何给定文本输出对应文本（搜索方法/解码策略）？
- 贪心搜索（greedy search）
- 集束搜索（beam search）

N-GRAM模型

RNN

总结：
1. 构建（训练）语言模型的过程就是学习词、句内在的“语言关系”；
2. 推理（预测）就是在给定上下文后，让构建的模型根据不同的解码策略输出对应的文本。
3. 无论训练还是预测，都以Token为粒度进行的。

## 1.3 ChatGPT基础
### 1.3.1 最强表示架构Transformer设计与演变
### 1.3.2 生成语言模型GTP进化与逆袭
### 1.3.3 利器强化学习RLHF流程与思想
## 1.4 本章小结



